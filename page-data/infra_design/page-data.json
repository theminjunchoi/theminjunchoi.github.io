{"componentChunkName":"component---src-templates-post-jsx","path":"/infra_design/","result":{"data":{"site":{"siteMetadata":{"title":"minjun.blog"}},"markdownRemark":{"id":"dec80c60-9cbd-54f2-8aae-62e15b54c78f","excerpt":"단일 인스턴스 목표 TPS 커피빵은 게임 기반 실시간 서비스이기 때문에 유저 간의 양방향 통신을 위해 웹소켓 통신을 이용하고 있다. 웹소켓 통신의 경우 서버가 각 클라이언트의 구독 상태를 세션별로 메모리에서 관리하고 있는데, REST API와는 다르게 연결이 지속적으로 유지되면서 이벤트 브로드캐스팅과 메시지 라우팅을 처리해야 한다. 이 과정에서 동시 접속…","html":"<h2>단일 인스턴스 목표 TPS</h2>\n<p>커피빵은 게임 기반 실시간 서비스이기 때문에 유저 간의 양방향 통신을 위해 웹소켓 통신을 이용하고 있다. 웹소켓 통신의 경우 서버가 각 클라이언트의 구독 상태를 세션별로 메모리에서 관리하고 있는데, REST API와는 다르게 연결이 지속적으로 유지되면서 이벤트 브로드캐스팅과 메시지 라우팅을 처리해야 한다. 이 과정에서 동시 접속자 수가 증가하면 메시지 처리 대기열이 쌓이게 되고, 결과적으로 응답 지연이 발생하는 메시지 처리 병목 현상이 생기게 된다.</p>\n<p>단일 인스턴스로 운영하던 서비스 초반, 하나의 EC2(AWS t4g.small)로 버틸 수 있는 처리량을 측정하기 위해 부하 테스트를 진행했다. 웹소켓 요청 처리(Inbound)와 응답 전송(Outbound) 각각의 스레드 풀 크기와 TPS를 점진적으로 증가시키며 테스트했고, 컨텍스트 스위칭 오버헤드로 인한 레이턴시 증가가 발생하기 직전 시점을 기준으로 아래와 같은 최적값을 도출했다.</p>\n<h4>Inbound 스레드 풀</h4>\n<ul>\n<li>코어, 최대 스레드 32개</li>\n<li>큐 크기 2048</li>\n<li>TPS 373 기준 P99 레이턴시 113ms</li>\n</ul>\n<h4>Outbound 스레드 풀</h4>\n<ul>\n<li>코어, 최대 스레드 16개</li>\n<li>큐 크기 4096</li>\n<li>TPS 2800 기준 P99 레이턴시 92ms</li>\n</ul>\n<p>이런 설정값을 유지했을 때 250명 정도가 동시 접속 가능한데, 더 많은 유저들이 몰리게 되면 어떻게 될까?</p>\n<h2>분산환경 구축 옵션</h2>\n<p>예산이 제한된 상황에서는 스케일 업(Scale-up)보다 스케일 아웃(Scale-out)을 우선 고려하게 된다. 그런데 여기서 추가적인 문제가 발생한다. 현재 이 서비스는 게임 세션 데이터를 DB가 아닌 <strong>인메모리(in-memory)</strong> 방식으로만 관리하고 있다. 즉, 같은 게임 세션에 참여하는 모든 유저는 <strong>동일한 인스턴스에 웹소켓 연결</strong>을 유지해야 한다.</p>\n<p>만약 아무 고려 없이 단순 스케일 아웃을 진행하면, 같은 게임 세션의 유저들이 서로 다른 인스턴스에 분산 연결될 수 있고, 이 경우 각 인스턴스가 서로 다른 게임 상태를 가지게 되어 <strong>게임 동기화가 깨지는 문제</strong>가 발생한다.</p>\n<p>이를 해결하기 위해 고려할 수 있는 방안은 다음과 같다.</p>\n<ol>\n<li><strong>MySQL을 활용한 실시간 상태 동기화</strong>: 모든 게임 상태 변경을 즉시 MySQL에 저장하고, 각 인스턴스가 DB를 조회하며 게임 진행</li>\n<li><strong>Redis를 원격 캐시로 사용</strong>: Redis를 공유 세션 저장소로 활용하여 모든 인스턴스가 동일한 게임 상태 참조</li>\n<li><strong>로컬 캐시 + Redis Pub/Sub 동기화</strong>: 각 인스턴스가 로컬 메모리에 캐시를 유지하되, Redis Pub/Sub을 통해 상태 변경 이벤트를 브로드캐스팅하여 동기화</li>\n</ol>\n<p>각 방안의 장단점을 비교하면 다음과 같다.</p>\n<h4>1. MySQL을 활용한 실시간 상태 동기화</h4>\n<p><strong>장점</strong></p>\n<ul>\n<li>데이터 영속성 보장. 서버 재시작 시에도 게임 상태 복구 가능</li>\n<li>별도 학습 곡선 없이 해결 가능</li>\n<li>트랜잭션 지원으로 데이터 정합성 확보</li>\n</ul>\n<p><strong>단점</strong></p>\n<ul>\n<li>매 액션마다 <strong>디스크 I/O 발생</strong>으로 레이턴시 급증. 실시간 게임에선 치명적</li>\n<li>동시 접속자 증가 시 DB 커넥션 풀 고갈 및 <strong>병목 발생</strong></li>\n<li>DB 부하 분산을 위해 결국 Read Replica나 샤딩 필요 → 복잡도 상승</li>\n</ul>\n<h4>2. Redis를 원격 캐시로 사용</h4>\n<p><strong>장점</strong></p>\n<ul>\n<li>디스크 I/O 없이 메모리에서 바로 조회하므로 MySQL 대비 빠름</li>\n<li>모든 인스턴스가 단일 Redis를 바라보므로 <strong>상태 일관성 자동 보장</strong></li>\n<li>세션 데이터 TTL 설정으로 자동 만료 처리 가능</li>\n</ul>\n<p><strong>단점</strong></p>\n<ul>\n<li>모든 요청이 네트워크를 타므로 로컬 메모리 대비 <strong>네트워크 레이턴시 존재</strong></li>\n<li><strong>매 요청마다 직렬화/역직렬화 오버헤드 발생</strong>. 특히 복잡한 객체 구조일수록 성능 저하 심각</li>\n<li>Redis 장애 시 전체 서비스 마비 → <strong>SPOF(Single Point of Failure)</strong> 위험</li>\n<li>Redis Cluster 구성 시 추가 비용 및 운영 복잡도 증가</li>\n</ul>\n<h4>3. 로컬 캐시 + Redis Pub/Sub 동기화</h4>\n<p><strong>장점</strong></p>\n<ul>\n<li>읽기는 로컬 메모리에서 처리 → <strong>가장 빠른 응답속도</strong>. 네트워크 통신 및 직렬화/역직렬화 불필요</li>\n<li>Redis는 동기화 메시지 전파 용도로만 사용하므로 부하 최소화</li>\n<li>Redis 일시 장애 시에도 로컬 캐시로 서비스 지속 가능</li>\n</ul>\n<p><strong>단점</strong></p>\n<ul>\n<li>Pub/Sub 메시지 전파 지연으로 <strong>일시적 데이터 불일치(Eventual Consistency)</strong> 발생 가능</li>\n<li>구현 복잡도 높음. 캐시 무효화(invalidation) 로직 정교하게 설계 필요</li>\n<li>메시지 유실 시 동기화 깨질 위험 존재</li>\n</ul>\n<h3>Redis를 원격 캐시로 사용</h3>\n<p>게임 특성상 유저 간 인터랙션이 빈번하게 발생하는데, 이를 MySQL 같은 RDBMS로 처리하기엔 레이턴시 측면에서 무리가 있었다. 또한 팀 내에서 Redis에 대한 사전 지식이 없어서, 학습 목적도 겸해 Redis 도입을 결정했다.</p>\n<p>2번과 3번 옵션 중 무엇을 선택할지 고민하다가, <strong>직접 구현해서 성능을 비교</strong>해보기로 하고 2번 옵션부터 적용했다.</p>\n<h4>동작 방식</h4>\n<p>모든 게임 세션 데이터를 <strong>중앙 집중식 Redis</strong>에 저장하고, 각 서버 인스턴스는 로컬 메모리에 데이터를 보관하지 않는 방식이다. 대신 게임 로직 실행 시 매번 Redis에서 데이터를 조회하고 수정한다.</p>\n<h4>처리 흐름 예시</h4>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">1. 유저 A가 \"카드 1번\" 선택 액션 전송\n2. 서버 1이 Redis에서 게임 세션 데이터 조회 후 역직렬화 → Java 객체 변환\n3. 비즈니스 로직 처리 (카드 효과 적용, 게임 상태 업데이트 등)\n4. 변경된 객체를 직렬화 → Redis에 저장\n5. 모든 서버 인스턴스가 변경된 Redis 데이터를 조회 및 역직렬화 후 연결된 유저들에게 브로드캐스팅</code></pre></div>\n<h4>문제점</h4>\n<p>이 방식의 가장 큰 문제는 <strong>직렬화/역직렬화가 빈번하게 발생</strong>한다는 점이다. 게임 로직 실행 중 매번 Redis를 거쳐야 하므로, 다음과 같은 오버헤드가 발생한다:</p>\n<ul>\n<li><strong>읽기 작업</strong>: Redis 조회 → 역직렬화 → 비즈니스 로직 실행</li>\n<li><strong>쓰기 작업</strong>: Redis 조회 → 비즈니스 로직 실행 → 직렬화 → Redis 저장</li>\n</ul>\n<p>특히 도메인 객체가 복잡할수록(중첩된 객체, 컬렉션 등) 직렬화 비용이 급격히 증가한다. 실제로 게임 세션 객체는 플레이어 리스트, 선택된 메뉴, 미니게임 상태 등 여러 계층의 데이터를 포함하고 있어, 한 번의 직렬화/역직렬화에 수 밀리초가 소요됐다.</p>\n<p>모든 구현을 완료하고 로컬 환경에서 약 300개의 테스트를 실행한 결과, <strong>기존 인메모리 방식 대비 2배 이상의 실행 시간</strong>이 소요되는 것을 확인했다. 실시간 게임에서 이 정도 성능 저하는 유저 경험에 직접적인 영향을 미치는 치명적인 문제였다.</p>\n<p>또한 이 구조는 <strong>Redis에 과도한 부하를 가한다</strong>는 문제도 있다. 모든 게임 로직이 Redis를 거쳐야 하므로, 동시 접속자가 증가하면 Redis가 애플리케이션 서버보다 먼저 병목이 될 가능성이 높다.</p>\n<h3>로컬 캐시 + Redis Pub/Sub 동기화</h3>\n<p>결국 우리는 3번으로 구현 방식을 바꿨다.</p>\n<h4>동작 방식</h4>\n<p>각 서버 인스턴스가 게임 세션 데이터를 <strong>로컬 메모리(인메모리 캐시)에 보관</strong>하고, 상태 변경이 발생하면 <strong>Redis Pub/Sub을 통해 다른 인스턴스들에게 변경 이벤트를 브로드캐스팅</strong>하는 방식이다. 각 인스턴스는 메시지를 수신하면 자신의 로컬 캐시를 업데이트한다.</p>\n<h4>처리 흐름 예시</h4>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">1. 유저 A가 \"카드 1번\" 선택 액션 전송 (서버 1에 연결)\n2. 서버 1이 로컬 메모리에서 게임 세션 데이터 조회 (역직렬화 불필요)\n3. 비즈니스 로직 처리 (카드 효과 적용, 게임 상태 업데이트)\n4. 서버 1이 변경 이벤트를 Redis Pub/Sub으로 발행 (Publish)\n5. 같은 게임 세션을 구독(Subscribe)하고 있는 서버 2, 3이 메시지 수신\n6. 각 서버가 자신의 로컬 캐시 업데이트 후 연결된 유저들에게 브로드캐스팅</code></pre></div>\n<h4>2번 방식과의 차이점</h4>\n<p>가장 큰 차이는 <strong>읽기 작업이 로컬 메모리에서 처리</strong>된다는 점이다.</p>\n<ul>\n<li>2번 방식: 매번 네트워크 통신 + 직렬화/역직렬화 필요</li>\n<li>3번 방식: 읽기는 로컬 메모리에서 즉시 처리, 쓰기 시에만 Pub/Sub 메시지 발행</li>\n</ul>\n<p>2번 방식은 게임 상태를 읽을 때도, 쓸 때도 항상 Redis를 거쳐야 한다. 반면 3번 방식은 <strong>데이터가 이미 로컬에 있기 때문에</strong> 비즈니스 로직 실행 중 발생하는 모든 읽기 작업이 네트워크 없이 바로 처리된다.</p>\n<h4>성능 개선 결과</h4>\n<p>동일한 300개 테스트 실행 결과, <strong>기존 인메모리 단일 인스턴스 방식과 거의 동일한 성능</strong>을 확인했다. 이벤트 발행과 읽는 시점에서 직렬화/역직렬화가 필요했지만, 그 크기가 2번의 상황보다 훨씬 적기 때문에 오버헤드가 적었다.</p>\n<h2>참고자료</h2>\n<h3>Redis 공식 자료</h3>\n<ul>\n<li><a href=\"https://redis.io/docs/latest/develop/pubsub/\">Redis Pub/Sub</a> - Pub/Sub 공식 가이드</li>\n<li><a href=\"https://redis.io/docs/latest/develop/reference/protocol-spec/\">RESP Protocol Specification</a> - RESP 프로토콜 공식 명세</li>\n<li><a href=\"https://github.com/redis/redis/blob/unstable/src/pubsub.c\">Redis GitHub - pubsub.c</a> - Pub/Sub 구현 소스코드</li>\n</ul>","frontmatter":{"title":"단일 서버에서 분산 환경으로: 확장성 있는 아키텍처로의 전환","date":"2025년 10월 13일 09:10","updated":"2026년 02월 18일 03:02","tags":["우아한테크코스","ZZOL"],"series":"ZZOL 개발록"},"fields":{"slug":"/infra_design/","readingTime":{"minutes":13.075}}},"seriesList":{"edges":[{"node":{"id":"dc0a8329-79ba-55f1-a688-c2f7cbe3f8fd","fields":{"slug":"/ideation/"},"frontmatter":{"title":"사이드 프로젝트에서 실제 서비스까지: ZZOL(커피빵) 기획과 그 시작"}}},{"node":{"id":"b8d473fd-79ee-51d9-ae2b-71c19291cb0e","fields":{"slug":"/persona/"},"frontmatter":{"title":"누가 우리 서비스를 쓸까? : 페르소나 정의를 통한 타겟 유저 구체화"}}},{"node":{"id":"17ea11c4-1553-5ec0-9e45-75f970e28462","fields":{"slug":"/how_spring_handles_websocket/"},"frontmatter":{"title":"Spring WebSocket 내부 동작 원리 파헤치기"}}},{"node":{"id":"9661b2d5-0110-5af9-aedb-86141ccf0b8d","fields":{"slug":"/websocket_reconnection_app_switching/"},"frontmatter":{"title":"모바일 백그라운드 전환 시에도 끊김 없는 WebSocket 연결 경험 만들기"}}},{"node":{"id":"d83add12-9f50-5d42-a371-f347258c1a37","fields":{"slug":"/thread_pool_tuning/"},"frontmatter":{"title":"스레드풀, 감으로 잡지 마세요: 부하 테스트로 증명하는 최적의 설정값"}}},{"node":{"id":"dec80c60-9cbd-54f2-8aae-62e15b54c78f","fields":{"slug":"/infra_design/"},"frontmatter":{"title":"단일 서버에서 분산 환경으로: 확장성 있는 아키텍처로의 전환"}}},{"node":{"id":"3fd12d05-6c82-5d61-91c5-732734d90aa2","fields":{"slug":"/how_redis_pubsub_works/"},"frontmatter":{"title":"Redis Pub/Sub을 활용한 다중 서버 간 실시간 메시지 동기화 전략"}}},{"node":{"id":"deb7290a-38ec-5ac8-a608-ff8d0903f9bb","fields":{"slug":"/coffeeshout_to_zzol/"},"frontmatter":{"title":"커피빵에서 ZZOL로, 유저가 알려준 진짜 서비스"}}},{"node":{"id":"cdc783fb-1df1-5d09-a998-412d8decc001","fields":{"slug":"/graceful_shutdown/"},"frontmatter":{"title":"WebSocket 서비스에서 Graceful Shutdown이 필요한 이유와 구현"}}},{"node":{"id":"624a9920-70ee-5dec-8f7c-0c62c15207de","fields":{"slug":"/zzol_query/"},"frontmatter":{"title":"쿼리 최적화, 36초를 1초로 줄이기까지"}}},{"node":{"id":"eb83fd57-b998-5c76-9ae1-ec3ab9709ee5","fields":{"slug":"/zzol_message_recovery/"},"frontmatter":{"title":"네트워크 불안정 상황에서도 메시지 유실 없는 견고한 게임 서버 설계"}}},{"node":{"id":"a015095a-309b-5c10-aaf5-2c59301c5ef3","fields":{"slug":"/zzol_circuit_breaker/"},"frontmatter":{"title":"외부 서비스 장애로부터 살아남기"}}}]},"previous":{"fields":{"slug":"/thread_pool_tuning/"},"frontmatter":{"title":"스레드풀, 감으로 잡지 마세요: 부하 테스트로 증명하는 최적의 설정값"}},"next":{"fields":{"slug":"/how_redis_pubsub_works/"},"frontmatter":{"title":"Redis Pub/Sub을 활용한 다중 서버 간 실시간 메시지 동기화 전략"}}},"pageContext":{"id":"dec80c60-9cbd-54f2-8aae-62e15b54c78f","series":"ZZOL 개발록","previousPostId":"d83add12-9f50-5d42-a371-f347258c1a37","nextPostId":"3fd12d05-6c82-5d61-91c5-732734d90aa2"}},"staticQueryHashes":[],"slicesMap":{}}